{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJrKCbjZsqpo"
   },
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-DU0hkyVyPi"
   },
   "source": [
    "# <font color=\"#003660\">Session 1: Document Classification/Regression with Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhy42GjRV3ON"
   },
   "source": [
    "# <font color=\"#003660\">Notebook 5: MLP with Word Embeddings as Features</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
    "        ... are able to train a neural network with word embeddings as features.\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram and equation below illustrate the architecture of a simple feedforward neural network, also known as a multi-layer perceptron (MLP) (James et al. 2023). The network consists of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next layer. The input layer has as many neurons as we have features and the output layer has one neuron in the case of regression and as many neurons as we have classes in the classification case. The number of neurons in the hidden layer is a hyperparameter that can be tuned by the analyst. The same applies to the number of hidden layers.\n",
    "\n",
    "<center><img width=512 src=\"https://raw.githubusercontent.com/olivermueller/amlta-2024/main/Session_02/mlp.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6vVpwIFsqps"
   },
   "source": [
    "# Import packages\n",
    "\n",
    "As always, we first need to load a number of required Python packages:\n",
    "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- `sklearn` is a free software machine learning library for the Python programming language.\n",
    "- `tensorflow` is an end-to-end open source platform for machine learning, especially deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mMrhkr83sqpt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDzj8w5e7DaY"
   },
   "source": [
    "Check if we are running on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FhHhJiZD6yQG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZd82t53sqpu"
   },
   "source": [
    "# Load documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load wine reviews (Source: https://www.kaggle.com/datasets/zynicide/wine-reviews) from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"https://raw.githubusercontent.com/olivermueller/amlta-2025/main/Session_01/winemag-data-130k-v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CureExnsIS-p"
   },
   "source": [
    "Split data into three sets: training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBpPCg5zILlu"
   },
   "outputs": [],
   "source": [
    "training = corpus.iloc[0:80000,].sample(n=10000) # sample to speed up training\n",
    "validation = corpus.iloc[80000:100000,]\n",
    "test = corpus.iloc[100000:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5Mygm5yIYnh"
   },
   "source": [
    "For each dataset, store features and targets in separate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18PF5d6ZIN-U"
   },
   "outputs": [],
   "source": [
    "train_corpus_features = training[[\"description\"]]\n",
    "train_corpus_target = training[[\"points\"]]\n",
    "val_corpus_features = validation[[\"description\"]]\n",
    "val_corpus_target = validation[[\"points\"]]\n",
    "test_corpus_features = test[[\"description\"]]\n",
    "test_corpus_target = test[[\"points\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvhh11WuIeyk"
   },
   "source": [
    "Create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas dataframes. The use of TensorFlow Datasets follows a common pattern:\n",
    "\n",
    "1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n",
    "2.   Apply transformations to preprocess the data in the dataset (e.g., tokenize and vectorize the texts).\n",
    "3. Iterate over the dataset and process its elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n",
    "\n",
    "Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOOszgPrQVVw"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n",
    "                                               tf.cast(train_corpus_target.values, tf.int32)))\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n",
    "                                             tf.cast(val_corpus_target.values, tf.int32)))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n",
    "                                              tf.cast(test_corpus_target.values, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXbjrq6NJ1Qk"
   },
   "source": [
    "Display some stats and examples from the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9LXPIY-XxDH"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"===\")\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwh4TWJbVFY9"
   },
   "source": [
    "# Vectorize documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM7gEBL8KHzg"
   },
   "source": [
    "We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors. **Instead of counting word appearances (like in the BOW model), we simply map words to integers (`output_mode = 'int'`) this time.** To get document representation of a fixed length, we limit the length of documents to 100 tokens (longer documents will be truncated, shorter documents will be padded with zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyJW-JetIN2w"
   },
   "outputs": [],
   "source": [
    "max_tokens = 10000\n",
    "max_length = 100\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4ElfvWtKj0Q"
   },
   "source": [
    "Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function (which can only be applied to the features (x) of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6xCAUAOMdYH"
   },
   "outputs": [],
   "source": [
    "train_ds_features_only = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(train_ds_features_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkQu4KlNLftU"
   },
   "source": [
    "Show the vocabulary that our vectorizer knows after being fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn0lt-7CRaCq"
   },
   "outputs": [],
   "source": [
    "text_vectorization.get_vocabulary()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjeZpApaLvyx"
   },
   "source": [
    "Next, we apply our `text_vectorization` function to all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHGn2peqMYuP"
   },
   "outputs": [],
   "source": [
    "vectorized_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "vectorized_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "vectorized_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_WzVu2WMEn-"
   },
   "source": [
    "Show results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etTA2eAUB6G"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in vectorized_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"===\")\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9QH9PcrSa7J"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_K1l5L-MQ7z"
   },
   "source": [
    "We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function). Note that we have to somehow reduce the dimensionality of the output of the embedding layer (sequence_length*embedding_dim). Here, we simply perform a global average pooling (i.e., average each element of the embedding vector over all tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMB14gBYSblz"
   },
   "outputs": [],
   "source": [
    "def get_model(hidden_dim=32):\n",
    "    inputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n",
    "    embedded = layers.Embedding(input_dim=max_tokens, output_dim=300, mask_zero=True)(inputs)\n",
    "    hidden1 = layers.GlobalAveragePooling1D()(embedded)\n",
    "    hidden2 = layers.Dense(hidden_dim, activation = \"relu\")(hidden1)\n",
    "    outputs = layers.Dense(1, activation = \"linear\")(hidden2)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer = tf.optimizers.Adam(),\n",
    "                  loss = \"mean_absolute_error\",\n",
    "                  metrics = [\"mean_absolute_error\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWZtLzduT_sH"
   },
   "source": [
    "Instantiate model and show it's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02bcqmcvTIDW"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W4SzaRqUExq"
   },
   "source": [
    "Fit model on training data and save best model to disk. We use the validation set to monitor how the performance of the model evolves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQZ8zbxbSzZb"
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"mlp.keras\", save_best_only=True)]\n",
    "\n",
    "history = model.fit(vectorized_train_ds.cache(),\n",
    "          validation_data = vectorized_val_ds.cache(),\n",
    "          epochs = 3,\n",
    "          batch_size = 128,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7NOcSDY0Y6I"
   },
   "source": [
    "Plot the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoHl0JaJ5fJm"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ347rt_Tx0G"
   },
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKR36ArkT2Gq"
   },
   "source": [
    "Load best model from training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA_ON4yBTb2a"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"mlp.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LRhBvwPT1q-"
   },
   "source": [
    "Make predictions on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yckI-BTWSTFm"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(vectorized_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMFi18uLT7tG"
   },
   "source": [
    "Calculate accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afIwDkWYS_i4"
   },
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(test_corpus_target, preds))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu",
     "timestamp": 1636383829898
    }
   ]
  },
  "kernelspec": {
   "display_name": "amlta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
