{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJrKCbjZsqpo"
   },
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-DU0hkyVyPi"
   },
   "source": [
    "# <font color=\"#003660\">Session 2: Transformer Architecture - Encoder-only Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhy42GjRV3ON"
   },
   "source": [
    "# <font color=\"#003660\">Notebook 4: Named Entity Recognition with Transformers</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
    "        ... you understand the differences between sequence and token classification, <br>\n",
    "        ... know how to fine-tune a NER model on labelled data, and <br>\n",
    "        ... upload and download a model to the HF hub.\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8WyaSVOepeR"
   },
   "source": [
    "The following content is heavily inspired by the following excellent sources:\n",
    "\n",
    "\n",
    "*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n",
    "*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euTwZ6Bne4E3"
   },
   "source": [
    "# Token vs. Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPnOBt7jDs5i"
   },
   "source": [
    "Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. Learn more: https://huggingface.co/learn/llm-course/chapter7/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8NxJHRgVp8H"
   },
   "source": [
    "The figures below show the main differences of using encoders like BERT and its sibblings and cousins for sequence and token classification. When doing **sequence classification** (e.g., sentiment analysis), we feed a sequence into the model and only work with the contextual embedding of the [CLS] token when training the classification head of the model. In contrast, when doing **token classification** we feed the contextual embeddings of all tokens through the classification head of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvBIe3QmXyS-"
   },
   "source": [
    "<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/seq_class.png\"/><br></center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/token_class.png\"/><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6vVpwIFsqps"
   },
   "source": [
    "# Import Packages\n",
    "\n",
    "As always, we first need to load a number of required Python packages:\n",
    "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. \n",
    "- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n",
    "- `datasets` and `evaluate` are libraries from Hugging Face to feed Transformers with data and evaluate their predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OFZTIYq-Nlbp"
   },
   "outputs": [],
   "source": [
    "#!pip install evaluate seqeval\n",
    "#!pip install datasets\n",
    "#!pip install datasets==2.19.1 # downward compatibility for datasets with scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mMrhkr83sqpt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import notebook_login, login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy8yT9LPEmQY"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB91kYyKZ1eB"
   },
   "source": [
    "In this notebook we will use the WNUT 17 dataset, which is a NER dataset focusing on emerging and rare entities. You can find more information about the dataset here: https://github.com/leondz/emerging_entities_17 and https://huggingface.co/datasets/manu/wnut_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut = load_dataset(\"manu/wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z9LjjdRwE2X3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilA4t0ydFMJQ"
   },
   "source": [
    "Each number in `ner_tags` represents a type of named entity (e.g., location, person). We can convert the numbers to textual labels to learn more about those entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K82K22FVFByE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kVuOiTWFJlj"
   },
   "source": [
    "The prefixes of the tags indicate whether a given token signifies the beginning or middle/end of a named entity:\n",
    "\n",
    "* **B-**: indicates the beginning of an entity.\n",
    "* **I-**: indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
    "* **O**: indicates that the token doesn’t correspond to any entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo8znf3rEuoJ"
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT87Q8TiGCrX"
   },
   "source": [
    "As always, the first thing to do when processing raw texts with Transformers is to tokenize the sequences. First, we need to load a tokenizer that is compatible with the architecture we want to use (here: DistilBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GRKsApeBEwAG"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi22MiALauGE"
   },
   "source": [
    "Let's re-join one example from the training set and feed it through the tokenizer. This mimics how we would use the tokenizer on new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eSPZ9yBbGOSa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(\" \".join(wnut[\"train\"][0][\"tokens\"]))\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ8fil5oGkeu"
   },
   "source": [
    "As expected, the tokenizer performed sub-word tokenization, splitting, for example, `ESB` into `es` and `##b`. In addition, the tokenizer added the usual `[CLS]` and `[SEP]` tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DihpmRxPbOA5"
   },
   "source": [
    "As a result, the tokenized sequence and the labels of our training data (which has NOT been tokenized with sub-word tokenization) are not aligend anymore. A quick look at the length of the two sequences confirms this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FzBLy_UhGboT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "944Ck3W_GdiY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wnut[\"train\"][0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7d_UuZFHJBA"
   },
   "source": [
    "Hence, we need to realign the tokens and labels using the following process:\n",
    "\n",
    "1. Map all tokens to their corresponding word with the `word_ids` method.\n",
    "2. Assig the label `-100` to the special tokens `[CLS]` and `[SEP]`, so that they can be ignored by the loss function.\n",
    "3. Only label the first token of a given word. Assign `-100` to other subtokens from the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgwsGOD6HRCA"
   },
   "source": [
    "Below is a function to tokenize the text and, afterwards, realign the tokens and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ncJI_vFSH1mf"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhwa2rcZbfi5"
   },
   "source": [
    "Apply the function to the whole dataset (i.e., train, validation, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PLkhx6YwH595"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a77aa1547c44f73bafb9f74c13930bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs9Lj5wSbmZb"
   },
   "source": [
    "Check whether the sequences are aligned now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9fNik2uPH94C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  1030,\n",
       "  2703,\n",
       "  17122,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  1996,\n",
       "  3193,\n",
       "  2013,\n",
       "  2073,\n",
       "  1045,\n",
       "  1005,\n",
       "  1049,\n",
       "  2542,\n",
       "  2005,\n",
       "  2048,\n",
       "  3134,\n",
       "  1012,\n",
       "  3400,\n",
       "  2110,\n",
       "  2311,\n",
       "  1027,\n",
       "  9686,\n",
       "  2497,\n",
       "  1012,\n",
       "  3492,\n",
       "  2919,\n",
       "  4040,\n",
       "  2182,\n",
       "  2197,\n",
       "  3944,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) == len(tokenized_wnut[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AP8vru5bwPu"
   },
   "source": [
    "To preprocess our texts on-the-fly while training our model, we can use a data collator (more information: https://huggingface.co/docs/transformers/main_classes/data_collator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NnPHiajMOBHD"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvQn7RWeKTMM"
   },
   "source": [
    "# Fine-tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_aHYIqjb4lv"
   },
   "source": [
    "Now we are (almost) ready to fine-tune a pre-trained DistilBERT on our emerging and rare named entities dataset. We will also see how to publish our model (checkpoint + tokenizer) on the Hugging Face dataset hub and download it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2cCrNtrcShN"
   },
   "source": [
    "As always, when training a model we need a compute_metrics function to calculate and display selected accuracy metrics during training. Here we use precision, recall, F1, and accuracy (they are all saved together in the seqeval evaluation object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vB4HeCstKjVH"
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WkJ8AMHcrDq"
   },
   "source": [
    "In order to be able to interpret the model's predictions, we also need two dictionaries to translate between the textual labels and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XcIof8DJK3Qu"
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLWqEVeVc04e"
   },
   "source": [
    "🚀 🚀 🚀 Now we are really ready to fine-tune... 🚀 🚀 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agR3A0wkc5EY"
   },
   "source": [
    "Load the pre-trained model, configure the classification head (how many labels?), and pass the dictionaries to translate between label IDs and textual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IFGaZp6dK6WQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7txAFUGdVGy"
   },
   "source": [
    "Configure the Trainer. Here, we will only train for 2 epochs. In reality, you probably want to train longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Yc1ifPK8N5Y6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/miniconda3/envs/amlta2025/lib/python3.10/site-packages/transformers/training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/4d/0rd3mwcn4y7gv39shh0dy_0w0000gn/T/ipykernel_47720/3444769125.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model_2025\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True, # Apple M compatibility\n",
    "    push_to_hub=False, # You need to be logged in for this\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJUieJIzdl-p"
   },
   "source": [
    "Go! 🏁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "m-7CvqpmT5tI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/426 03:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.577963</td>\n",
       "      <td>0.257646</td>\n",
       "      <td>0.356410</td>\n",
       "      <td>0.938566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270022</td>\n",
       "      <td>0.575410</td>\n",
       "      <td>0.325301</td>\n",
       "      <td>0.415631</td>\n",
       "      <td>0.941943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/miniconda3/envs/amlta2025/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=426, training_loss=0.20419825522552634, metrics={'train_runtime': 226.1982, 'train_samples_per_second': 30.009, 'train_steps_per_second': 1.883, 'total_flos': 91781128898820.0, 'train_loss': 0.20419825522552634, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTc0_H2rT_2-"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKwFkGbCUkEc"
   },
   "source": [
    "## Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqKA8Kpzdz-K"
   },
   "source": [
    "In the following cells, we instantiate a pipeline, which integrates a trained model (aka checkpoint) and compatible tokenizer, and feed it with a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WLiCAVjzPQCJ"
   },
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WwSox4_2ZAud"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-group',\n",
       "  'score': 0.21721636,\n",
       "  'index': 1,\n",
       "  'word': 'the',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.45459718,\n",
       "  'index': 2,\n",
       "  'word': 'golden',\n",
       "  'start': 4,\n",
       "  'end': 10},\n",
       " {'entity': 'I-location',\n",
       "  'score': 0.21033008,\n",
       "  'index': 3,\n",
       "  'word': 'state',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'B-group',\n",
       "  'score': 0.24908938,\n",
       "  'index': 4,\n",
       "  'word': 'warriors',\n",
       "  'start': 17,\n",
       "  'end': 25},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.6600871,\n",
       "  'index': 13,\n",
       "  'word': 'san',\n",
       "  'start': 80,\n",
       "  'end': 83},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.51444423,\n",
       "  'index': 14,\n",
       "  'word': 'francisco',\n",
       "  'start': 84,\n",
       "  'end': 93}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeVcDvp7UmIJ"
   },
   "source": [
    "## By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GePBKSCvVAaX"
   },
   "source": [
    "Instead of using a pipeline, we can also compute predictions step-by-step. Reproducing each of the steps by hand ✍️ will increase your understanding of the underlying logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxFSyooQeO1V"
   },
   "source": [
    "Tokenize the example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Hod_oPTKUtcr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3585, 2110, 6424, 2024, 2019, 2137, 2658, 3455, 2136, 2241,\n",
       "         1999, 2624, 3799, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIfVUJ66eTwq"
   },
   "source": [
    "Do a forward pass through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3n76RB6lUwoE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0330e+00, -2.5403e-01,  1.0945e-01, -8.0330e-01, -9.2720e-02,\n",
       "           4.5146e-01,  1.7608e-02, -2.9176e-01,  2.0122e-01, -4.1523e-01,\n",
       "          -4.4312e-01, -2.3184e-01, -3.9708e-01],\n",
       "         [ 1.1797e+00, -5.6219e-01, -7.0985e-01, -6.2261e-01, -1.0430e+00,\n",
       "           1.2617e+00,  8.2001e-01,  7.0649e-01,  2.4967e-01, -6.2227e-01,\n",
       "          -8.8441e-01, -6.7178e-01, -7.3283e-01],\n",
       "         [ 2.4666e-01, -5.6814e-01, -7.7910e-01, -4.8853e-01, -1.0387e+00,\n",
       "           1.0034e+00, -4.9541e-02,  2.3482e+00,  1.2157e+00, -1.7934e-01,\n",
       "          -8.6357e-01, -6.9077e-01, -7.4703e-01],\n",
       "         [ 9.0998e-01, -7.3500e-01, -5.5140e-01, -9.8822e-01, -9.0076e-01,\n",
       "           1.0711e+00,  4.8070e-01,  7.8460e-01,  1.2280e+00, -6.7979e-01,\n",
       "          -7.6542e-01, -9.5512e-01, -8.7403e-01],\n",
       "         [ 9.7002e-01, -7.4336e-01, -7.7352e-01, -9.5438e-01, -1.0250e+00,\n",
       "           1.4820e+00,  1.2102e+00,  3.4165e-01,  6.2862e-01, -4.8289e-01,\n",
       "          -3.5293e-01, -6.8561e-01, -7.1134e-01],\n",
       "         [ 2.9837e+00, -5.4299e-01, -3.3932e-01, -1.1842e+00, -7.6007e-01,\n",
       "           9.5297e-01,  7.5753e-01, -1.0895e-01, -3.9663e-01, -6.9866e-01,\n",
       "          -3.8068e-01, -7.3941e-01, -8.5310e-01],\n",
       "         [ 2.8491e+00, -3.6203e-01, -3.6510e-01, -1.1023e+00, -8.8101e-01,\n",
       "           1.0177e+00,  5.3599e-01,  2.8073e-01, -3.7659e-01, -5.6196e-01,\n",
       "          -5.2472e-01, -8.0890e-01, -7.3389e-01],\n",
       "         [ 2.3435e+00, -5.4975e-01, -4.6185e-01, -1.3348e+00, -1.0498e+00,\n",
       "           1.0394e+00, -2.7971e-01,  7.6563e-01,  5.9009e-01, -1.5490e-01,\n",
       "          -1.0008e+00, -6.8288e-01, -5.7310e-01],\n",
       "         [ 2.7349e+00, -3.3720e-01, -4.5713e-01, -9.3874e-01, -8.9232e-01,\n",
       "           1.1199e+00,  4.9767e-01,  6.0725e-02, -2.2948e-02, -2.3806e-01,\n",
       "          -8.4601e-01, -5.9573e-01, -7.2842e-01],\n",
       "         [ 2.8037e+00, -3.4098e-01, -4.1670e-01, -9.2540e-01, -7.4116e-01,\n",
       "           8.3476e-01,  5.8744e-01, -1.7556e-03, -4.6868e-01,  3.3207e-01,\n",
       "          -5.9165e-01, -3.4426e-01, -3.4610e-01],\n",
       "         [ 2.5383e+00, -3.0096e-01, -1.2352e-01, -8.9463e-01, -8.9416e-01,\n",
       "           9.5075e-01,  8.6558e-01,  1.1617e-01, -1.4851e-01, -3.1961e-01,\n",
       "          -7.2869e-01, -4.8669e-01, -7.6204e-01],\n",
       "         [ 2.8367e+00, -4.2324e-01, -2.9886e-01, -8.1303e-01, -8.3748e-01,\n",
       "           8.3433e-01,  1.8476e-01,  4.5612e-01, -7.8455e-02, -3.5659e-01,\n",
       "          -8.2333e-01, -5.7659e-01, -7.2929e-01],\n",
       "         [ 2.1461e+00, -9.3857e-01, -2.0986e-01, -6.4620e-01, -1.0412e+00,\n",
       "           1.0052e+00, -3.7647e-01,  1.3227e+00,  5.5084e-01, -2.3414e-01,\n",
       "          -9.1093e-01, -8.1197e-01, -1.0165e+00],\n",
       "         [ 2.6537e-01, -6.1438e-01, -4.0894e-01, -6.4062e-01, -1.1648e+00,\n",
       "           6.6649e-01, -2.5538e-01,  3.1360e+00,  1.3950e+00, -4.4681e-01,\n",
       "          -1.2196e+00, -8.7225e-01, -9.3579e-01],\n",
       "         [ 4.3615e-01, -4.6417e-01, -6.2512e-01, -1.0181e+00, -8.8705e-01,\n",
       "           6.0092e-01,  3.1822e-02,  2.6685e+00,  1.7004e+00, -4.6142e-01,\n",
       "          -8.7881e-01, -8.9374e-01, -1.0831e+00],\n",
       "         [ 1.9762e+00, -2.6553e-01,  7.0350e-02, -4.3545e-02, -8.7414e-02,\n",
       "           3.2363e-01,  7.2309e-02,  3.0671e-01, -4.2935e-01, -5.0969e-01,\n",
       "          -6.1285e-01, -2.0600e-01, -6.2753e-02],\n",
       "         [ 2.6781e+00, -4.0889e-01, -2.8274e-01, -2.8773e-01, -3.3667e-01,\n",
       "           3.8845e-01,  1.1672e-01,  3.3210e-01, -9.9788e-02, -6.8949e-01,\n",
       "          -9.0322e-01, -4.8475e-01, -4.6652e-01]]], device='mps:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RpWM84ReYBn"
   },
   "source": [
    "For each token, get the index of the label with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5FuJWnhJUx4o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5, 7, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0]], device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(logits, dim=2)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMtc-L1celF5"
   },
   "source": [
    "Translate the indices to textual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yrsyEd6nUypj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-group',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-location',\n",
       " 'B-location',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "predicted_token_class"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "V-DU0hkyVyPi"
   ],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu",
     "timestamp": 1636383829898
    }
   ]
  },
  "kernelspec": {
   "display_name": "amlta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
