{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJrKCbjZsqpo"
   },
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-DU0hkyVyPi"
   },
   "source": [
    "# <font color=\"#003660\">Session 2: Transformer Architecture - Encoder-only Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhy42GjRV3ON"
   },
   "source": [
    "# <font color=\"#003660\">Notebook 2: Text Classification with Transformers</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
    "        ... will know the difference between the feature extraction and fine.-tuning approach for text classification with Transformers, <br>\n",
    "        ... will know how to train a feature extraction model for text classification, <br>\n",
    "        ... will know how to train a fine-tuned model for text classification, <br>,\n",
    "        ... will get to know the main libraries of the Hugging Face ecosystem (i.e., Datasets, Tokenizers, Transformers).\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPvSfoxIngjS"
   },
   "source": [
    "The following content is heavily inspired by the following excellent sources:\n",
    "\n",
    "\n",
    "*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n",
    "*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6vVpwIFsqps"
   },
   "source": [
    "# Import Packages\n",
    "\n",
    "As always, we first need to load a number of required Python packages:\n",
    "- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n",
    "- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "- `sklearn` is a free software machine learning library for the Python programming language.\n",
    "- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n",
    "- `datasets` is an API for datasets from the makers of transformers.\n",
    "- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mMrhkr83sqpt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF9NcQIOF38V"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTmR0vobGR0l"
   },
   "source": [
    "<center><br><img width=400 src=\"https://upload.wikimedia.org/wikipedia/en/f/f1/Bert_and_Ernie.JPG\"/><br></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l-voX_EF5Qw"
   },
   "source": [
    "\n",
    "\n",
    "In this notebook we will use a famous Transformer model called **BERT**, short for Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "We will use the three core libraries from the Hugging Face ecosystem: **Datasets**, **Tokenizers**, and **Transformers**. These libraries will allow us to quickly go from raw text to a fine-tuned model that can be used for predictions on new texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates the architecture of a BERT model for sequence classification at a high level:\n",
    "\n",
    "<center><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/amlta-2024/main/Session_03/bert.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oufSV6RpCvd"
   },
   "source": [
    "# Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeEBELLPGp2C"
   },
   "source": [
    "In the next code chunks we will use the **Datasets** library to retrieve a dataset from Hugging Face's datasets hub. This library is designed to load and process large datasets efficiently and share them with the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSlhyYVxG-Hb"
   },
   "source": [
    "Load the \"emotions\" dataset, a dataset of English Twitter messages labelled with six basic emotions: anger, fear, joy, love, sadness, and surprise. More: https://huggingface.co/datasets/emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rwJVxFPvpzT_"
   },
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByhadfPTHYz-"
   },
   "source": [
    "Explore the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B6SAG2pbp7Qs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "faDfu2M0qMKz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqsbo3kyHb4b"
   },
   "source": [
    "Show the first Tweet and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YZMHgTVzqCyq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax0-WcUpHeQY"
   },
   "source": [
    "Get the name of the first label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3z2F2bsg7oGM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].features[\"label\"].int2str(emotions[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIByOSDzHpsb"
   },
   "source": [
    "Store a list of all label names for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WF8gOTUb8pA5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YhYjl4xqyTV"
   },
   "source": [
    "# Tokenize Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lSWAfhxHuD8"
   },
   "source": [
    "Like other neural networks or machine learning models, Transformer models cannot process raw strings as input; instead they assume the text has been tokenized and turned into numerical vectors.\n",
    "\n",
    "Most Transformers use a subword **tokenizer**. The idea behind subword tokenization is a meet-in-the-middle between character and word tokenization. On one hand we want to use characters since they allow the model to deal with rare character combinations and misspellings. On the other hand, we want to keep frequent words and word parts as unique entities.\n",
    "\n",
    "There are many different subword tokenization strategies. Using the right tokenizer for a given pretrained model is crucial for getting sensible results. The Transformers library provides convenient tools (e.g., AutoTokenizer, from_pretrained()) to load both objects (model and tokenizer) from the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8d8UYAoNrY2w"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IFsJMcZ8rgq1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gKvG2UEtrjrt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9khZudEcrl8j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 8552, 22199, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_str = tokenizer.encode(\"this is a complicatedtest\")\n",
    "encoded_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6rkFNq4JLUt"
   },
   "source": [
    "Show special tokens used by the tokenizer. BERT uses the [MASK] token for the primary objective of masked language modeling and the [CLS] and [SEP] tokens for the secondary pretraining objective of predicting if two sentences are consecutive.\n",
    "\n",
    "Below, we can observe two things. First, the [CLS] and [SEP] tokens have been added automatically to the start and end of the sequence. Second, the long word `complicatedtest` has been split into two tokens. The ## prefix in ##test signifies that the preceding string is not a whitespace and that it should be merged with the previous token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ilmFb1RVrpvi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 [CLS]\n",
      "2023 this\n",
      "2003 is\n",
      "1037 a\n",
      "8552 complicated\n",
      "22199 ##test\n",
      "102 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_str:\n",
    "    print(token, tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYL980hHJtUH"
   },
   "source": [
    "# BERT for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N9njIOlJvmQ"
   },
   "source": [
    "BERT has been pretrained to predict masked words and next sentences in texts. Hence, we can’t use the model as-is for text classification and have to modify its architecture slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAIRsBuyJ97p"
   },
   "source": [
    "The figure below illustrates the general architecture of a BERT text classification model.\n",
    "\n",
    "<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/bert.png\"/><br></center>\n",
    "\n",
    "First, the text is tokenized and represented as one-hot vectors whose dimension is the size of the tokenizer vocabulary (not shown in the diagram). Next, these token encodings are embedded in lower dimensions and passed through stacks of encoder layers to yield a hidden state for each input token. During pretraining, the hidden states are used for the task of language modeling. For the classification task, we replace the language modeling layer with a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uykU2peUJ-L0"
   },
   "source": [
    "We have two options to train the classification model:\n",
    "\n",
    "* Feature extraction: We use BERT in inference mode take the hidden states as features to train an \"external\" classifier on them.\n",
    "\n",
    "* Fine-tuning: We add a classification head to the model and train the whole model end-to-end, which also updates the parameters of the pretrained BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr_LiWpCot7a"
   },
   "source": [
    "# The Feature Extraction Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A20HvS5ELJYY"
   },
   "source": [
    "The figure below illustrates the idea behind the feature extraction approach. To use a Transformer as a feature extractor we use BERT just for inference and use the generated hidden states as features for a downstream classifier (e.g., logistic regression, random forest). The advantage of this approach is that we can quickly train a shallow model. This method is especially convenient if GPUs are unavailable since the hidden states can be computed relatively fast on a CPU.\n",
    "\n",
    "<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/feature_extraction.png\"/><br></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlWiBzYenTgy"
   },
   "source": [
    "See also Jay Alammar's blog post for using the feature-extraction approach for classification: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4nqFlbRrBnV"
   },
   "source": [
    "## Model: Load and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr0CvfmlL2AC"
   },
   "source": [
    "Check if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GeXhxwclsHcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw0E67odL4_R"
   },
   "source": [
    "(Down)Load a pretrained model and move it to the GPU. The AutoModel class initalizes an input encoder that translates the one-hot vectors to embeddings with positional encodings and feeds them through the encoder stack to return the hidden states. Note that the language model head that normally takes the hidden states and decodes them to the masked token prediction is excluded since it is only needed for pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ANW4qnDiZD-D"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h33LOIMc9hs7"
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vigbRRZNMBiC"
   },
   "source": [
    "Test the model with a single short document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fwgGorPo9oh5"
   },
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "text_tensor = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized text is a tensor with the dimensionality [1, 6] (number of documents, length of the documents). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ldSpgvWM-aaN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the tensor are integers, which are the indices of the tokens in the vocabulary (101 and 102 are special tokens that have been added during tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2023, 2003, 1037, 3231,  102]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed the tensor through the model to get the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hm3nltvTCL80"
   },
   "outputs": [],
   "source": [
    "output = model(text_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
       "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
       "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
       "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
       "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
       "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output are the hidden states of the last encoder layer for each token. Hence, they have the dimensionality [1, 6, 768] (number of documents, length of the documents, dimension of the hidden states). They are the raw contextual embeddings generated by BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VQQ8GqjxMjUt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fxvRM5_3MeO9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
       "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
       "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
       "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
       "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
       "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80xF_JyEMmyU"
   },
   "source": [
    "Let’s repeat the process for the whole dataset. First, we write a simple function that will tokenize our texts. The padding=True parameter will pad the examples with zeroes to the longest one in a batch, and truncation=True will truncate the examples to the model’s maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kccbnYLuCzEa"
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlhVecn-NEFz"
   },
   "source": [
    "Apply to the first three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BfVQMr2ZCzHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102], [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(emotions[\"train\"][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSALNDtpNG-g"
   },
   "source": [
    "Repeat with the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bf7zFgfQCzKk"
   },
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRV7TMy-NMCy"
   },
   "source": [
    "The above function call has added two new features to the dataset: input_ids and the attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5EKgbeMCCzNw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']),\n",
       " 'input_ids': List(Value('int32')),\n",
       " 'attention_mask': List(Value('int8'))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY3E9ADUNfh6"
   },
   "source": [
    "With `hidden_states = model(input_ids, attention_mask)` we could now generate the hidden states for each token of each document. For convenience, we define a custom function that takes a batch of tokenized documents, feeds them through the model, and adds hidden_state features to the batch. Instead of using the hidden states of all tokens, in the original BERT paper it was recommended to only use the hidden states of the CLS token as features for classification. An alternative would be to average over the hidden states of all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BelGgNdUiLr4"
   },
   "outputs": [],
   "source": [
    "def extract_features_cls(batch):\n",
    "  # store inputs in separate variables\n",
    "  input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "  attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "\n",
    "  # feed inputs into model and save outputs\n",
    "  with torch.no_grad():\n",
    "      last_hidden_state = model(input_ids, attention_mask).last_hidden_state\n",
    "      last_hidden_state = last_hidden_state.cpu().numpy()\n",
    "\n",
    "  # extract the hidden state of the CLS token (i.e., the first token)\n",
    "  cls_lhs = last_hidden_state[:,0,:]\n",
    "\n",
    "  # return results\n",
    "  batch[\"cls_hidden_state\"] = cls_lhs\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zDeUKiXLFSb-"
   },
   "outputs": [],
   "source": [
    "emotions_encoded = emotions_encoded.map(extract_features_cls,\n",
    "                                        batched=True,\n",
    "                                        batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg6CbLybN4jX"
   },
   "source": [
    "Show the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HTo6ZpqACzTX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']),\n",
       " 'input_ids': List(Value('int32')),\n",
       " 'attention_mask': List(Value('int8')),\n",
       " 'cls_hidden_state': List(Value('float32'))}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnFfhrX3OiNW"
   },
   "source": [
    "Check the dimensions of the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zX4m0yQdFYvC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotions_encoded[\"train\"][\"cls_hidden_state\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF4W231oN9bL"
   },
   "source": [
    "The dataset now contains all the information we need to train a classifier on it. We will use the extracted hidden states of the CLS tokens as input features and the labels as targets. We can easily create the corresponding arrays in the well known Scikit-Learn format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "U6xd4jmy-XHr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 768), (2000, 768))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(emotions_encoded[\"train\"][\"cls_hidden_state\"])\n",
    "X_valid = np.array(emotions_encoded[\"validation\"][\"cls_hidden_state\"])\n",
    "y_train = np.array(emotions_encoded[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_encoded[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWdarGxMOKDD"
   },
   "source": [
    "And now we can train a standard classifier on these data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TKhnWJMkuZrX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=3000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">3000</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=3000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=3000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d98HpqT9rIP-"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4xoQYscOSbB"
   },
   "source": [
    "Make predictions on validation set and evaluate predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8tRNCCJOut7O"
   },
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aG93pkwkukIv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.65      0.71      0.68       550\n",
      "         joy       0.71      0.80      0.75       704\n",
      "        love       0.49      0.30      0.37       178\n",
      "       anger       0.51      0.44      0.47       275\n",
      "        fear       0.55      0.56      0.55       212\n",
      "    surprise       0.54      0.27      0.36        81\n",
      "\n",
      "    accuracy                           0.63      2000\n",
      "   macro avg       0.57      0.51      0.53      2000\n",
      "weighted avg       0.62      0.63      0.62      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5wxuEsgoxOK"
   },
   "source": [
    "# The Fine-tuning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH6OjPg-Ol-S"
   },
   "source": [
    "Let’s now fine-tune a Transformer end-to-end. With the fine-tuning approach we do not use the hidden states as fixed features, but instead train them as shown in the figure below. Since we retrain all the parameters, this approach requires more compute than the feature extraction approach and typically requires a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTMO-UzOOyNN"
   },
   "source": [
    "<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/fine_tuning.png\"/><br></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uamJyUWArP2C"
   },
   "source": [
    "## Model: Load Pre-trained Model and Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8-IdNAfPCIk"
   },
   "source": [
    "In the following, we will use the Trainer API to simplify the training loop.\n",
    "\n",
    "We need is a pretrained model like the one we used in the feature-based approach. The only difference is that we use the AutoModelForSequenceClassification model instead of AutoModel. This model has a classification head on top of the model outputs which can be easily trained with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nTUjcMuLZFNl"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "kXlWZARF3FZy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_name, num_labels = 6)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjvcMv6NP8mk"
   },
   "source": [
    "Furthermore, we define a function to monitor some metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gE0ePh_4P85A"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOM8x2gYP4lU"
   },
   "source": [
    "With the dataset and metrics ready we can now instantiate a Trainer class. The main ingredient here is the TrainingArguments class to specify all the parameters of the training run, one of which is the output directory for the model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PWTgI6Dv3ZvD"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"results\",\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=3e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  report_to=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESgn9VNIQaeS"
   },
   "source": [
    "We can now instantiate and fine-tune our model with the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XdiUuJsP3oz7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/miniconda3/envs/amlta2025/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 05:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.226784</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.921907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.935557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/miniconda3/envs/amlta2025/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4320345458984375, metrics={'train_runtime': 339.6264, 'train_samples_per_second': 94.221, 'train_steps_per_second': 1.472, 'total_flos': 720342861696000.0, 'train_loss': 0.4320345458984375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQYRp0-9rQT7"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D82JH6g9Qicy"
   },
   "source": [
    "Make predictions on validation set and evaluate predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "KEsMi6xu6tBa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/miniconda3/envs/amlta2025/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 4.92795   , -0.7383838 , -1.1149226 , -0.9638449 , -1.2058758 ,\n",
       "        -1.7956127 ],\n",
       "       [ 4.8994846 , -0.925503  , -1.6025988 , -0.89169985, -0.7663219 ,\n",
       "        -1.6876588 ],\n",
       "       [-1.3763558 ,  2.5984538 ,  3.007816  , -1.0662369 , -2.2169294 ,\n",
       "        -1.8496767 ],\n",
       "       ...,\n",
       "       [-1.0316542 ,  4.997706  ,  0.1024612 , -1.0043087 , -1.5416032 ,\n",
       "        -1.620951  ],\n",
       "       [-1.6368203 ,  3.7184246 ,  2.434468  , -1.4215966 , -2.308291  ,\n",
       "        -1.5715662 ],\n",
       "       [-1.2465651 ,  5.05565   , -0.16913122, -1.5041523 , -1.3891387 ,\n",
       "        -0.5576928 ]], dtype=float32), label_ids=array([0, 0, 2, ..., 1, 1, 1]), metrics={'test_loss': 0.17003321647644043, 'test_accuracy': 0.9355, 'test_f1': 0.9355569427561781, 'test_runtime': 4.6277, 'test_samples_per_second': 432.177, 'test_steps_per_second': 6.915})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Rvr5d1Qn0D"
   },
   "source": [
    "Above, you can see that the model outputs are raw logits, not probabilities. Nonetheless, we can simply take the highest value as the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "IRHVLH_j67Hj"
   },
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "K92-yKIq62Wa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.96      0.97      0.96       550\n",
      "         joy       0.97      0.94      0.95       704\n",
      "        love       0.87      0.88      0.87       178\n",
      "       anger       0.93      0.95      0.94       275\n",
      "        fear       0.88      0.88      0.88       212\n",
      "    surprise       0.86      0.85      0.86        81\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.91      0.91      0.91      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vBFltOiR_zj"
   },
   "source": [
    "The classification report reveals that the fine-tune approach is performing much better than the feature extraction approach."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu",
     "timestamp": 1636383829898
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "amlta2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
